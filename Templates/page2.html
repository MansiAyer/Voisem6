<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
  <title>Emotion Detector</title>
  <style type="text/css">
    /*https://analyticsindiamag.com/wp-content/uploads/2019/12/spectrograms-scaled.jpg*/
  </style>
</head>
<body style="background:url(/static/spectrogram-graphic.png); background-repeat: no-repeat; background-size: cover; background-color: black; background-attachment:fixed;  background-blend-mode: hard-light;">


<nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom border-info" style="height: 90px;">
  <div class="container-fluid">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav" style="">
      <ul class="navbar-nav text-center" style="margin-left: 55px; width: 90%;">
        <li class="nav-item" >
          <a class="nav-link fs-4" href="/">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link fs-4" href="/pagetwo">Features</a>
        </li>
        <li class="nav-item">
          <a class="nav-link fs-4 pr-5" href="/pagethree">Pricing</a>
        </li>
      </ul>
    </div>
  </div>
</nav>


<div class="container pb-5 flex flex-col" style="height: 450px;">
  <h1 class="display-2 text-light border-bottom border-light" style="position: absolute; bottom: 300px; font-size: 80px; ">Features and design of the project</h1>
  <!--<img src="https://samplecraze.com/wp-content/uploads/2019/01/wordscansoun-609x321.jpg" class="img-fluid" alt="...">-->
</div>

<div class="container p-5 border border-info text-center rounded-3" style="opacity: 0.8;">
  <div class="container rounded-top" style="width: 100%; right: 170px; padding-top: 10px; padding-bottom: 35px; background-color: rgba(171,213,255,0.7);">
    <h2 class="pb-3 border-bottom border-light text-white">About the project</h2>
    <p class="pt-3 text-white container" style="">Speech transcriptions are widely used in various sentiment analysis applications. In emotion detection, it is important for a model to understand the context of the utterance correctly to be able to predict its intent accurately. Let us take the word “good” for example. Used on its own it is hard to know the context in which this word has been used. Although the word “good” implies something positive, it could have possibly been part of a larger conversation and could have been used in a sarcastic way.
      Our model makes it possible to detect such underlying emotions from a voice recording, thus making it easier to execute sentiment analysis applications.
    </p>
  </div>
  <div class="container bg-info" style="width: 100%; right: 170px; padding-bottom: 35px; padding-top: 35px;">
    <h2 class="pb-3 border-bottom border-light text-light">ML model used</h2>
    <p class="pt-3 text-light container" style="">Deep learning approaches and principles such as CNNs and Long Short Term Memory (LSTM) cells have been successfully applied to speech features in recent studies on speech processing. Extensive research has shown that CNNs are very useful in extracting information from raw signals in a variety of applications, including speech recognition, image recognition, and so on. We use Spectrograms and MFCCs, which are widely used to describe speech features, as well as CNNs for emotion detection, in our research. The CNN model described in this section takes speech transcriptions, in the form of word embeddings, as input to detect emotion. CNNs can directly be applied to word embeddings.
    </p>
  </div>
  <div class="container bg-primary rounded-bottom" style="width: 100%; right: 170px; padding-bottom: 35px; padding-top: 35px;">
    <h2 class="pb-3 border-bottom border-light text-light">Audio Feature Extraction</h2>
    <p class="pt-3 text-light container" style="">The Mel-frequency Spectrogram is used as an input to a 2D CNN . When the Short Term Fourier Transform (STFT) is applied to a windowed audio or speech signal, spectrograms are generated. The audio is sampled at a frequency of 22050 Hz. A “hann” window with a length of 2048 is then used to window each audio frame.Spectrograms are generated when the Short Term Fourier Transform (STFT) is applied to a windowed audio or speech signal. The audio is sampled at a 22050 Hz frequency. Each audio frame is then windowed using a “hann” window with a length of 2048.</p><p class="pt text-light"> The said windowed audio samples are then subjected to Fast Fourier Transform (FFT) windows of length 2048 with an STFT hop-length of 512.Mel-spectrograms are generated by mapping the obtained Spectrogram magnitudes to the Mel-scale. This model employs 128 spectrogram coefficients per window. The Mel-frequency scale emphasizes the lower end of the frequency continuum over the higher ones, simulating human hearing perceptual abilities. The Mel-spectrograms were computed using the "librosa" python package and the above-mentioned parameters.</p><p class="pt text-light"> A sample Mel-Spectrogram corresponding to the first audio of Actor 1.</p>
    <img class="p-2 border border-light" src="/static/MelSpec_FirstAudio.png">
    <p class="pt-3 text-light"> A sample Waveplot corresponding to the first audio of Actor 1.</p>
    <img class="p-2 border border-light" src="/static/Waveplot_FirstAudio.png">
  </div>
  <!--<div class="container" style="width: 100%; right: 170px; padding-bottom: 35px; padding-top: 35px;">
    <h2 class="pb-3 border-bottom border-light text-white">ML mode</h2>
    <p class="pt-3 text-white container" style="">Deep learning approaches and principles such as CNNs and Long Short Term Memory (LSTM) cells have been successfully applied to speech features in recent studies on speech processing. Extensive research has shown that CNNs are very useful in extracting information from raw signals in a variety of applications, including speech recognition, image recognition, and so on. We use Spectrograms and MFCCs, which are widely used to describe speech features, as well as CNNs for emotion detection, in our research. The CNN model described in this section takes speech transcriptions, in the form of word embeddings, as input to detect emotion. CNNs can directly be applied to word embeddings.
    </p>
  </div>-->
</div>
<div style="height: 100px;"></div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>
<script src="http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script type="text/javascript" src="//code.jquery.com/jquery-1.9.1.js"></script>
<script type="text/javascript" src="//code.jquery.com/ui/1.9.2/jquery-ui.js"></script>
<script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
<script>
  /*setTimeout(() => { 
    document.getElementById("demo").innerHTML = "Model Loaded. Record live or upload a pre-recorded clip";
  }, 2000)*/
</script>
</body>
</html>
